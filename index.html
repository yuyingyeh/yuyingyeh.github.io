<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yu-Ying Yeh</title>
  
  <meta name="author" content="Yu-Ying Yeh">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="shortcut icon" href="favicon.ico">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yu-Ying Yeh</name>
              </p>
              <p>I am a Ph.D. student advised by Prof. <a target="_blank" href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a> at <a target="_blank" href="http://jacobsschool.ucsd.edu/visualcomputing/index.shtml">UC San Diego</a>.
              </p>
              <p>I am working on computer vision problems related to photorealistic content creation for AR applications.</p> 
              <p>I am fortunate to work with Dr. <a target="_blank" href="http://www.kalyans.org/">Kalyan Sunkavalli</a> at Adobe Research and Dr. <a target="_blank" href="https://mingyuliu.net/">Ming-Yu Liu</a> at NVIDIA Research during my internship.</p>
              <p>Before joining UCSD, I was a research assistant working with Prof. <a target="_blank" href="https://scholar.google.com/citations?user=HSGvdtoAAAAJ&hl=en" >Yu-Chiang Frank Wang</a> at National Taiwan University.</p>
              <p style="text-align:center">
                <a target="_blank" href="mailto:yuyeh@eng.ucsd.edu">Email</a> &nbsp/&nbsp
                <a target="_blank" href="data/Resume_YuYing_202306.pdf">CV</a> &nbsp/&nbsp
                <a target="_blank" href="https://scholar.google.com.tw/citations?user=b0YKBxgAAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="https://github.com/yuyingyeh/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a target="_blank" href="images/YuYing2.png"><img style="width:80%;max-width:80%" alt="profile photo" src="images/YuYing2.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interest mainly focuses on the intersection of computer vision and computer graphics, including but not limited to inverse rendering, 3D reconstruction, material and lighting estimation for indoor scene, object, and portrait.   
              My goal is to enable photorealistic content creation automatically for AR/VR applications. 
              Besides, I am also interested in representation learning, feature disentanglement and video prediction and have done some works related to generative models on videos and domain adaptation.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="lumos_stop()" onmouseover="lumos_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <div class="two" id='lumos_image'><img src='images/lumos.jpeg' width="110%"></div>
                <img src='images/lumos.jpeg' width="110%">
                </div>
                <script type="text/javascript">
                function lumos_start() {
                    document.getElementById('lumos_image').style.opacity = "1";
                }

                function lumos_stop() {
                    document.getElementById('lumos_image').style.opacity = "0";
                }
                lumos_stop()
                </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                            <a target="_blank" href="https://research.nvidia.com/labs/dir/lumos/">
                <papertitle>Learning to Relight Portrait Images via a Virtual Light Stage and Synthetic-to-Real Adaptation</papertitle>
                </a>
                <br>
                <strong>Yu-Ying Yeh</strong>,  
                <a target="_blank" href="https://luminohope.org/">Koki Nagano</a>, 
                <a target="_blank" href="https://www.samehkhamis.com/">Sameh Khamis</a>, 
                <a target="_blank" href="https://jankautz.com/">Jan Kautz</a>, 
                <a target="_blank" href="http://mingyuliu.net/">Ming-Yu Liu</a>,
                <a target="_blank" href="https://tcwang0509.github.io/">Ting-Chun Wang</a>
                <br>
                <em>SIGGRAPH Asia</em>, 2022  
                <br>
                <a target="_blank" href="https://research.nvidia.com/labs/dir/lumos/">project page</a> / 
                <a target="_blank" href="https://arxiv.org/abs/2209.10510">arxiv</a> / 
                <a target="_blank" href="https://youtu.be/uWSVpG0eKbU">video</a>
                <!-- <a target="_blank" href="http://imaginaire.cc/Lumos/">demo</a> -->
                <p></p>
                <p>We propose a single-image portrait relighting method trained with our rendered dataset and synthetic-to-real adaptation to achieve high photorealism without using light stage data. Our method can also handle eyeglasses and support video relighting.</p>
            </td>
          </tr>
          <tr onmouseout="photoscene_stop()" onmouseover="photoscene_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='photoscene_image'><img src='images/photoscene.gif' width="110%"></div>
                <img src='images/photoscene.jpg' width="110%">
              </div>
              <script type="text/javascript">
                function photoscene_start() {
                  document.getElementById('photoscene_image').style.opacity = "1";
                }

                function photoscene_stop() {
                  document.getElementById('photoscene_image').style.opacity = "0";
                }
                photoscene_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a target="_blank" href="https://yuyingyeh.github.io/projects/photoscene.html">
                <papertitle>PhotoScene: Photorealistic Material and Lighting Transfer for Indoor Scenes</papertitle>
              </a>
              <br>
              <strong>Yu-Ying Yeh</strong>,  
              <a target="_blank" href="https://sites.google.com/a/eng.ucsd.edu/zhengqinli">Zhengqin Li</a>, 
              <a target="_blank" href="https://yannickhold.com/">Yannick Hold-Geoffroy</a>, 
              <a target="_blank" href="https://jerrypiglet.github.io/">Rui Zhu</a>, 
              <a target="_blank" href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu</a>, <br>
              <a target="_blank" href="http://www.miloshasan.net/">Miloš Hašan</a>, 
              <a target="_blank" href="http://www.kalyans.org/">Kalyan Sunkavalli</a>, 
              <a target="_blank" href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a>
              <br>
              <em>CVPR</em>, 2022  
              <br>
              <a target="_blank" href="https://yuyingyeh.github.io/projects/photoscene.html">project page</a> / 
              <a target="_blank" href="http://arxiv.org/abs/2207.00757">arXiv</a> /
              <a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yeh_PhotoScene_Photorealistic_Material_and_Lighting_Transfer_for_Indoor_Scenes_CVPR_2022_paper.pdf">cvpr paper</a> / 
              <a target="_blank" href="https://github.com/ViLab-UCSD/PhotoScene">code</a>
              <p></p>
              <p>Transfer high-quality procedural materials and lightings from images to reconstructed indoor scene 3D geometry, which enables photorealistic 3D content creation for digital twins.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/openrooms_teaser.png' width="110%">
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a target="_blank" href="https://vilab-ucsd.github.io/ucsd-openrooms/">
                <papertitle>OpenRooms: An Open Framework for Photorealistic Indoor Scene Datasets</papertitle>
              </a>
              <br>
              <a target="_blank" href="https://sites.google.com/a/eng.ucsd.edu/zhengqinli">Zhengqin Li</a>, 
              Ting-Wei Yu, Shen Sang, Sarah Wang, 
              <a target="_blank" href="https://sites.google.com/site/mengsong1130/">Meng Song</a>, 
              Yuhan Liu, 
              <strong>Yu-Ying Yeh</strong>, <br>
              <a target="_blank" href="https://jerrypiglet.github.io/">Rui Zhu</a>, 
              <a target="_blank" href="https://scholar.google.com/citations?user=v19p_0oAAAAJ&hl=en">Nitesh Gundavarapu</a>, 
              Jia Shi, 
              <a target="_blank" href="https://sai-bi.github.io/">Sai Bi</a>, 
              <a target="_blank" href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu</a>, 
              <a target="_blank" href="https://kovenyu.com/">Hong-Xing Yu</a>, 
              <a target="_blank" href="http://www.kalyans.org/">Kalyan Sunkavalli</a>, 
              <a target="_blank" href="http://www.miloshasan.net/">Miloš Hašan</a>, 
              <a target="_blank" href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a target="_blank" href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a>
              <br>
				<em>CVPR</em>, 2021 &nbsp <font color=#d0176d><strong>(Oral Presentation)</strong></font>
              <br>
                <a target="_blank" href="https://vilab-ucsd.github.io/ucsd-openrooms/">project page</a> / 
                <a target="_blank" href="https://arxiv.org/pdf/2007.12868.pdf">arXiv</a> 	
              <p></p>
              <p>An open framework which creates a large-scale photorealistic indoor scene dataset OpenRooms from a publicly available video scans dataset ScanNet.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/cvpr20_teaser.png' width="110%">
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a target="_blank" href="https://cseweb.ucsd.edu//~viscomp/projects/CVPR20Transparent/">
                <papertitle>Through the Looking Glass: Neural 3D Reconstruction of Transparent Shapes</papertitle>
              </a>
              <br>
              <strong>Yu-Ying Yeh</strong><sup>*</sup>,  
              <a target="_blank" href="https://sites.google.com/a/eng.ucsd.edu/zhengqinli">Zhengqin Li</a><sup>*</sup>, 
              <a target="_blank" href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a> (*equal contributions)
              <br>
				<em>CVPR</em>, 2020 &nbsp <font color=#d0176d><strong>(Oral Presentation)</strong></font>
              <br>
                <a target="_blank" href="https://cseweb.ucsd.edu//~viscomp/projects/CVPR20Transparent/">project page</a> / 
                <a target="_blank" href="https://arxiv.org/abs/2004.10904">arXiv</a> / 
                <a target="_blank" href="https://github.com/lzqsd/TransparentShapeReconstruction">code</a> / 
                <a target="_blank" href="https://github.com/lzqsd/TransparentShapeDataset">dataset</a> /
                <a target="_blank" href="https://github.com/yuyingyeh/TransparentShapeRealData">real data</a> 			
              <p></p>
              <p>Transparent shape reconstruction from multiple images captured from a mobile phone.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/Video_Completion.png' width="110%">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9099414">
                <papertitle>Static2Dynamic: Video Inference from a Deep Glimpse</papertitle>
              </a>
              <br>              
              <strong>Yu-Ying Yeh</strong>, 
              <a target="_blank" href="https://ycliu93.github.io/">Yen-Cheng Liu</a>, 
              <a target="_blank" href="https://walonchiu.github.io/">Wei-Chen Chiu</a>, 
              <a target="_blank" href="https://scholar.google.com/citations?user=HSGvdtoAAAAJ&hl=en" >Yu-Chiang Frank Wang</a>
              <br>
				<em>IEEE Transactions on Emerging Topics in Computational Intelligence</em>, 2020
              <br>
              <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9099414">paper</a> /
              <a target="_blank" href="data/yeh2020static2dynamic.bib">bibtex</a>
              <p></p>
              <p>Video generation, interpolation, inpainting and prediction given a set of anchor frames.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/UFDN.png' width="110%">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1809.01361">
                <papertitle>A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation</papertitle>
              </a>
              <br>              
              <a target="_blank" href="https://alexander-h-liu.github.io/">Alexendar Liu</a>, 
              <a target="_blank" href="https://ycliu93.github.io/">Yen-Cheng Liu</a>, 
              <strong>Yu-Ying Yeh</strong>, 
              <a target="_blank" href="https://scholar.google.com/citations?user=HSGvdtoAAAAJ&hl=en" >Yu-Chiang Frank Wang</a>
              <br>
				<em>NeurIPS</em>, 2018
              <br>
              <a target="_blank" href="https://arxiv.org/abs/1809.01361">arXiv</a> /
              <a target="_blank" href="data/liu2018unified.bib">bibtex</a> /
              <a target="_blank" href="https://github.com/Alexander-H-Liu/UFDN">code</a>
              <p></p>
              <p>A novel and unified deep learning framework which is capable of learning domain-invariant representation from data across multiple domains.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/CDRD.png' width="110%">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Detach_and_Adapt_CVPR_2018_paper.pdf">
                <papertitle>Detach and Adapt: Learning Cross-Domain Disentangled Deep Representation</papertitle>
              </a>
              <br>
              <a target="_blank" href="https://ycliu93.github.io/">Yen-Cheng Liu</a>, 
              <strong>Yu-Ying Yeh</strong>, 
              Tzu-Chien Fu, 
              <a target="_blank" href="https://walonchiu.github.io/">Wei-Chen Chiu</a>, <br>
              Sheng-De Wang, 
              <a target="_blank" href="https://scholar.google.com/citations?user=HSGvdtoAAAAJ&hl=en" >Yu-Chiang Frank Wang</a>
              <br>
				<em>CVPR</em>, 2018 &nbsp <font color=#d0176d><strong>(Spotlight Presentation)</strong></font>
              <br>
              <a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Detach_and_Adapt_CVPR_2018_paper.pdf">paper</a> /
              <a target="_blank" href="data/liu2018detach.bib">bibtex</a> /
              <a target="_blank" href="https://github.com/ycliu93/CDRD">code</a> /
              <a target="_blank" href="https://youtu.be/sIkUzmgUaxc?t=4148">presentation</a>
              <p></p>
              <p>Feature disentanglement for cross-domain data which enables image translation and manipulation from labeled source doamin to unlabeled target domain.</p>
            </td>
          </tr>

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/conf_logo.png" width="100%"></td>
            <td width="75%" valign="center">
              Reviewer: 
              ICCV ’19, AAAI ’20, CVPR ’20, ECCV ’20, NeurIPS ’20, ICLR ’21,
                CVPR ’21, ICCV’21, NeurIPS’21, CVPR’22, ECCV’22, NeurIPS'22, Computer Graphics Forum
              <br>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/icon/ucsd.png" width="100%"></td>
            <td width="75%" valign="center">
              Teaching Assistant: <br> 
              Intro to Computer Vision: 
              <a target="_blank" href="https://haosulab.github.io/intro-cv/WI22/index.html">CSE152A WI22</a>, 
              <a target="_blank" href="https://cseweb.ucsd.edu/classes/sp19/cse152-a/">CSE152A SP19</a>, 
              <a target="_blank" href="https://cseweb.ucsd.edu/~mkchandraker/classes/CSE152A/Winter2019/">CSE152A WI19</a> <br>
              Advanced Computer Vision: 
              <a target="_blank" href="https://cseweb.ucsd.edu/~mkchandraker/classes/CSE252D/Spring2021/">CSE252D SP21</a> <br>
              Domain Adaptation in Computer Vision: 
              <a target="_blank" href="https://cseweb.ucsd.edu/~mkchandraker/classes/CSE291/Winter2020/">CSE291A00 WI20</a> <br>
              <br>
              Honors and Awards: <br>
              <a target="_blank" href="https://research.facebook.com/blog/2022/2/announcing-the-recipients-of-the-2022-meta-phd-research-fellowship/">2022 Meta PhD Research Fellowship Finalist</a><br>
              <a target="_blank" href="https://www.qualcomm.com/research/university-relations/innovation/fellowship/2022-north-america">2022 Qualcomm Innovative Fellowship Finalist</a><br>
              <a target="_blank" href="https://research.google/outreach/phd-fellowship/recipients/">2022 Google PhD Fellowship Recipient</a> [<a target="_blank" href="https://cse.ucsd.edu/about/news/yu-ying-yeh-phd-24-cses-newest-recipient-prestigious-google-fellowship">CSE News</a>]
              
              <br>
            </td>
          </tr>
          
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                This page is adapted from this <a target="_blank" href="https://github.com/jonbarron/jonbarron_website">template</a> which is created by <a target="_blank" href="http://jonbarron.info">Jon Barron</a>.
              </p>
              <p style="text-align:center;font-size:small;">
                
                <a target="_blank" href="https://clustrmaps.com/site/1b0l1" title="Visit tracker"><img src="https://www.clustrmaps.com/map_v2.png?d=sI3uXC17N02IXnLan89Vlgs6IFTeeVZFpS9l1mALqN0&cl=786b6f&co=ffffff&ct=808080&cmo=cdaa9d&cmn=d6818d"></a>
                
                <!-- 
                <a href="https://clustrmaps.com/site/1b0l1" title="Visit tracker"> </a>
                -->
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
