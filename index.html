<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yu-Ying Yeh</title>

    <meta name="author" content="Yu-Ying Yeh">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yu-Ying Yeh
                </p>
                <p style="text-align: justify;">I am a final-year Ph.D. candidate working with Prof. <a target="_blank" href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a> at <a target="_blank" href="http://jacobsschool.ucsd.edu/visualcomputing/index.shtml">UC San Diego</a>.
                I am fortunate to intern at <strong>Adobe Research, NVIDIA Research, and Meta Reality Lab</strong> during my Ph.D. study.
                Before joining UCSD, I was a research assistant working with Prof. <a target="_blank" href="https://scholar.google.com/citations?user=HSGvdtoAAAAJ&hl=en" >Yu-Chiang Frank Wang</a> at National Taiwan University.
                </p>
                <p style="text-align: justify;">
                    My research interest focuses on computer vision and graphics, with applications related to photorealistic 3D content creation, inverse rendering, and neural rendering.
                </p>
                <p style="text-align: justify;">
                    I'm honored to receive the <a target="_blank" href="https://cse.ucsd.edu/about/news/yu-ying-yeh-phd-24-cses-newest-recipient-prestigious-google-fellowship">Google PhD Fellowship</a> and be a finalist for the Meta PhD Research Fellowship and Qualcomm Innovative Fellowship.
                </p>
                <p style="color: #d0176d" style="text-align: justify;"><strong>I'm actively looking for full-time positions starting from mid-2024. 
                </strong>
                </p>
                <p style="text-align:center">
                  <a target="_blank" href="mailto:yuyeh@ucsd.edu">Email</a> &nbsp/&nbsp
                  <a target="_blank" href="data/Resume_YuYing_202402.pdf">CV</a> &nbsp/&nbsp
                  <a target="_blank" href="https://scholar.google.com.tw/citations?user=b0YKBxgAAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                  <a target="_blank" href="https://github.com/yuyingyeh/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/YuYing2.png"><img style="width:80%;max-width:80%" alt="profile photo" src="images/YuYing2.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interest mainly focuses on the intersection of computer vision and computer graphics, including but not limited to inverse rendering, 3D reconstruction, material and lighting estimation for indoor scene, object, and portrait.   
              My goal is to enable photorealistic content creation automatically for AR/VR applications. 
              <!-- Besides, I am also interested in representation learning, feature disentanglement and video prediction and have done some works related to generative models on videos and domain adaptation. -->
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr onmouseout="texturedreamer_stop()" onmouseover="texturedreamer_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                        <div class="two" id='texturedreamer_video'><video  width=110% muted autoplay loop>
                        <source src="https://texturedreamer.github.io/videos/plush_bear0_sofa_0_ours.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                        </video></div>
                        <img src='images/texturedreamer_teaser.jpg' width="110%">
                        <!-- <source src="https://texturedreamer.github.io/videos/plush_bear0_imgs_input.mp4" type="video/mp4"> -->
                    </div>
                    <!-- <div class="one">
                    <div class="two" id='texturedreamer_image'><img src='https://texturedreamer.github.io/static/images/teaser.jpg' width="110%"></div>
                    <img src='https://texturedreamer.github.io/static/images/teaser.jpg' width="110%">
                    % https://texturedreamer.github.io/videos/plush_bear0_imgs_input.mp4
                    </div> -->
                    <script type="text/javascript">
                    function texturedreamer_start() {
                        document.getElementById('texturedreamer_video').style.opacity = "1";
                    }
    
                    function texturedreamer_stop() {
                        document.getElementById('texturedreamer_video').style.opacity = "0";
                    }
                    texturedreamer_stop()
                    </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                                <a target="_blank" href="https://texturedreamer.github.io">
                    <span class="papertitle">TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion</span>  
                  </a>
                    <br>
                    <strong>Yu-Ying Yeh</strong>,  
                    <a target="_blank" rel="noopener noreferrer" href="https://jbhuang0604.github.io">Jia-Bin Huang</a>, 
                    <a target="_blank" rel="noopener noreferrer" href="https://changilkim.com">Changil Kim</a>, 
                    <a target="_blank" rel="noopener noreferrer" href="https://leixiao-ubc.github.io">Lei Xiao</a>, 
                    <a target="_blank" rel="noopener noreferrer" href="https://www.monkeyoverflow.com/about">Thu Nguyen-Phuoc</a>,
                    <a target="_blank" rel="noopener noreferrer" href="https://nkhan2.github.io">Numair Khan</a>,
                    <a target="_blank" rel="noopener noreferrer" href="https://holmes969.github.io">Cheng Zhang</a>,
                    <a target="_blank" rel="noopener noreferrer" href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a>,
                    <a target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=xWD7ZRkAAAAJ&sortby=pubdate">Carl S Marshall</a>,
                    <a target="_blank" rel="noopener noreferrer" href="http://flycooler.com">Zhao Dong</a>,
                    <a target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/citations?hl=zh-CN&user=Nxc2RbQAAAAJ&view_op=list_works&sortby=pubdate">Zhengqin Li</a>
                    <br>
                    <em>CVPR</em>, 2024  
                    <br>
                    <a target="_blank" href="https://texturedreamer.github.io">project page</a> / 
                    <a target="_blank" href="https://arxiv.org/abs/2401.09416">arxiv</a>
                    <!-- <a target="_blank" href="https://youtu.be/uWSVpG0eKbU">video</a> -->
                    <!-- <a target="_blank" href="http://imaginaire.cc/Lumos/">demo</a> -->
                    <p></p>
                    <p>TextureDreamer transfers photorealistic, high-fidelity, 
                        and geometry-aware textures from 3-5 images to arbitrary 3D meshes. </p>
                </td>
              </tr>
            
            <tr onmouseout="lumos_stop()" onmouseover="lumos_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                  <div class="two" id='lumos_image'><img src='images/lumos.jpeg' width="110%"></div>
                  <img src='images/lumos.jpeg' width="110%">
                  </div>
                  <script type="text/javascript">
                  function lumos_start() {
                      document.getElementById('lumos_image').style.opacity = "1";
                  }
  
                  function lumos_stop() {
                      document.getElementById('lumos_image').style.opacity = "0";
                  }
                  lumos_stop()
                  </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                              <a target="_blank" href="https://research.nvidia.com/labs/dir/lumos/">
                  <span class="papertitle">Learning to Relight Portrait Images via a Virtual Light Stage and Synthetic-to-Real Adaptation</span>  
                </a>
                  <br>
                  <strong>Yu-Ying Yeh</strong>,  
                  <a target="_blank" href="https://luminohope.org/">Koki Nagano</a>, 
                  <a target="_blank" href="https://www.samehkhamis.com/">Sameh Khamis</a>, 
                  <a target="_blank" href="https://jankautz.com/">Jan Kautz</a>, 
                  <a target="_blank" href="http://mingyuliu.net/">Ming-Yu Liu</a>,
                  <a target="_blank" href="https://tcwang0509.github.io/">Ting-Chun Wang</a>
                  <br>
                  <em>SIGGRAPH Asia</em>, 2022  
                  <br>
                  <a target="_blank" href="https://research.nvidia.com/labs/dir/lumos/">project page</a> / 
                  <a target="_blank" href="https://arxiv.org/abs/2209.10510">arxiv</a> / 
                  <a target="_blank" href="https://youtu.be/uWSVpG0eKbU">video</a>
                  <!-- <a target="_blank" href="http://imaginaire.cc/Lumos/">demo</a> -->
                  <p></p>
                  <p>We propose a single-image portrait relighting method trained with our rendered dataset and synthetic-to-real adaptation to achieve high photorealism without using light stage data. Our method can also handle eyeglasses and support video relighting.</p>
              </td>
            </tr>
    
            <tr onmouseout="photoscene_stop()" onmouseover="photoscene_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='photoscene_image'><video  width=110% muted autoplay loop>
                  <source src="images/photoscene.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/photoscene.jpg' width="110%">
                </div>
                <script type="text/javascript">
                  function photoscene_start() {
                    document.getElementById('photoscene_image').style.opacity = "1";
                  }
  
                  function photoscene_stop() {
                    document.getElementById('photoscene_image').style.opacity = "0";
                  }
                  photoscene_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a target="_blank" href="https://yuyingyeh.github.io/projects/photoscene.html">
                  <span class="papertitle">PhotoScene: Photorealistic Material and Lighting Transfer for Indoor Scenes</span>
                </a>
                <br>
                <strong>Yu-Ying Yeh</strong>,  
                <a target="_blank" href="https://sites.google.com/a/eng.ucsd.edu/zhengqinli">Zhengqin Li</a>, 
                <a target="_blank" href="https://yannickhold.com/">Yannick Hold-Geoffroy</a>, 
                <a target="_blank" href="https://jerrypiglet.github.io/">Rui Zhu</a>, 
                <a target="_blank" href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu</a>, <br>
                <a target="_blank" href="http://www.miloshasan.net/">Miloš Hašan</a>, 
                <a target="_blank" href="http://www.kalyans.org/">Kalyan Sunkavalli</a>, 
                <a target="_blank" href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a>
                <br>
                <em>CVPR</em>, 2022  
                <br>
                <a target="_blank" href="https://yuyingyeh.github.io/projects/photoscene.html">project page</a> / 
                <a target="_blank" href="http://arxiv.org/abs/2207.00757">arXiv</a> /
                <a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yeh_PhotoScene_Photorealistic_Material_and_Lighting_Transfer_for_Indoor_Scenes_CVPR_2022_paper.pdf">cvpr paper</a> / 
                <a target="_blank" href="https://github.com/ViLab-UCSD/PhotoScene">code</a>
                <p></p>
                <p>Transfer high-quality procedural materials and lightings from images to reconstructed indoor scene 3D geometry, which enables photorealistic 3D content creation for digital twins.</p>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/openrooms_teaser.png' width="110%">
                </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <a target="_blank" href="https://vilab-ucsd.github.io/ucsd-openrooms/">
                    <span class="papertitle">OpenRooms: An Open Framework for Photorealistic Indoor Scene Datasets</span>
                </a>
                <br>
                <a target="_blank" href="https://sites.google.com/a/eng.ucsd.edu/zhengqinli">Zhengqin Li</a>, 
                Ting-Wei Yu, Shen Sang, Sarah Wang, 
                <a target="_blank" href="https://sites.google.com/site/mengsong1130/">Meng Song</a>, 
                Yuhan Liu, 
                <strong>Yu-Ying Yeh</strong>, <br>
                <a target="_blank" href="https://jerrypiglet.github.io/">Rui Zhu</a>, 
                <a target="_blank" href="https://scholar.google.com/citations?user=v19p_0oAAAAJ&hl=en">Nitesh Gundavarapu</a>, 
                Jia Shi, 
                <a target="_blank" href="https://sai-bi.github.io/">Sai Bi</a>, 
                <a target="_blank" href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu</a>, 
                <a target="_blank" href="https://kovenyu.com/">Hong-Xing Yu</a>, 
                <a target="_blank" href="http://www.kalyans.org/">Kalyan Sunkavalli</a>, 
                <a target="_blank" href="http://www.miloshasan.net/">Miloš Hašan</a>, 
                <a target="_blank" href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
                <a target="_blank" href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a>
                <br>
          <em>CVPR</em>, 2021 &nbsp <font color=#d0176d><strong>(Oral Presentation)</strong></font>
                <br>
                  <a target="_blank" href="https://vilab-ucsd.github.io/ucsd-openrooms/">project page</a> / 
                  <a target="_blank" href="https://arxiv.org/pdf/2007.12868.pdf">arXiv</a> 	
                <p></p>
                <p>An open framework which creates a large-scale photorealistic indoor scene dataset OpenRooms from a publicly available video scans dataset ScanNet.</p>
              </td>
            </tr>
  
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/cvpr20_teaser.png' width="110%">
                </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a target="_blank" href="https://cseweb.ucsd.edu//~viscomp/projects/CVPR20Transparent/">
                  <span class="papertitle">Through the Looking Glass: Neural 3D Reconstruction of Transparent Shapes</span>
                </a>
                <br>
                <strong>Yu-Ying Yeh</strong><sup>*</sup>,  
                <a target="_blank" href="https://sites.google.com/a/eng.ucsd.edu/zhengqinli">Zhengqin Li</a><sup>*</sup>, 
                <a target="_blank" href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a> (*equal contributions)
                <br>
          <em>CVPR</em>, 2020 &nbsp <font color=#d0176d><strong>(Oral Presentation)</strong></font>
                <br>
                  <a target="_blank" href="https://cseweb.ucsd.edu//~viscomp/projects/CVPR20Transparent/">project page</a> / 
                  <a target="_blank" href="https://arxiv.org/abs/2004.10904">arXiv</a> / 
                  <a target="_blank" href="https://github.com/lzqsd/TransparentShapeReconstruction">code</a> / 
                  <a target="_blank" href="https://github.com/lzqsd/TransparentShapeDataset">dataset</a> /
                  <a target="_blank" href="https://github.com/yuyingyeh/TransparentShapeRealData">real data</a> 			
                <p></p>
                <p>Transparent shape reconstruction from multiple images captured from a mobile phone.</p>
              </td>
            </tr>
  
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/Video_Completion.png' width="110%">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9099414">
                  <span class="papertitle">Static2Dynamic: Video Inference from a Deep Glimpse</span>
                </a>
                <br>              
                <strong>Yu-Ying Yeh</strong>, 
                <a target="_blank" href="https://ycliu93.github.io/">Yen-Cheng Liu</a>, 
                <a target="_blank" href="https://walonchiu.github.io/">Wei-Chen Chiu</a>, 
                <a target="_blank" href="https://scholar.google.com/citations?user=HSGvdtoAAAAJ&hl=en" >Yu-Chiang Frank Wang</a>
                <br>
          <em>IEEE Transactions on Emerging Topics in Computational Intelligence</em>, 2020
                <br>
                <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9099414">paper</a> /
                <a target="_blank" href="data/yeh2020static2dynamic.bib">bibtex</a>
                <p></p>
                <p>Video generation, interpolation, inpainting and prediction given a set of anchor frames.</p>
              </td>
            </tr>
  
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/UFDN.png' width="110%">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a target="_blank" href="https://arxiv.org/abs/1809.01361">
                  <span class="papertitle">A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation</span>
                </a>
                <br>              
                <a target="_blank" href="https://alexander-h-liu.github.io/">Alexendar Liu</a>, 
                <a target="_blank" href="https://ycliu93.github.io/">Yen-Cheng Liu</a>, 
                <strong>Yu-Ying Yeh</strong>, 
                <a target="_blank" href="https://scholar.google.com/citations?user=HSGvdtoAAAAJ&hl=en" >Yu-Chiang Frank Wang</a>
                <br>
          <em>NeurIPS</em>, 2018
                <br>
                <a target="_blank" href="https://arxiv.org/abs/1809.01361">arXiv</a> /
                <a target="_blank" href="data/liu2018unified.bib">bibtex</a> /
                <a target="_blank" href="https://github.com/Alexander-H-Liu/UFDN">code</a>
                <p></p>
                <p>A novel and unified deep learning framework which is capable of learning domain-invariant representation from data across multiple domains.</p>
              </td>
            </tr>
  
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/CDRD.png' width="110%">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Detach_and_Adapt_CVPR_2018_paper.pdf">
                  <span class="papertitle">Detach and Adapt: Learning Cross-Domain Disentangled Deep Representation</span>
                </a>
                <br>
                <a target="_blank" href="https://ycliu93.github.io/">Yen-Cheng Liu</a>, 
                <strong>Yu-Ying Yeh</strong>, 
                Tzu-Chien Fu, 
                <a target="_blank" href="https://walonchiu.github.io/">Wei-Chen Chiu</a>, <br>
                Sheng-De Wang, 
                <a target="_blank" href="https://scholar.google.com/citations?user=HSGvdtoAAAAJ&hl=en" >Yu-Chiang Frank Wang</a>
                <br>
          <em>CVPR</em>, 2018 &nbsp <font color=#d0176d><strong>(Spotlight Presentation)</strong></font>
                <br>
                <a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Detach_and_Adapt_CVPR_2018_paper.pdf">paper</a> /
                <a target="_blank" href="data/liu2018detach.bib">bibtex</a> /
                <a target="_blank" href="https://github.com/ycliu93/CDRD">code</a> /
                <a target="_blank" href="https://youtu.be/sIkUzmgUaxc?t=4148">presentation</a>
                <p></p>
                <p>Feature disentanglement for cross-domain data which enables image translation and manipulation from labeled source doamin to unlabeled target domain.</p>
              </td>
            </tr>

          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/conf_logo.png" width="100%"></td>
              <td width="75%" valign="center">
                Reviewer: 
                ICCV ’19, AAAI ’20, CVPR ’20, ECCV ’20, NeurIPS ’20, ICLR ’21,
                  CVPR ’21, ICCV’21, NeurIPS’21, CVPR’22, ECCV’22, NeurIPS'22, Computer Graphics Forum
                <br>
              </td>
            </tr>
  
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/icon/ucsd.png" width="100%"></td>
              <td width="75%" valign="center">
                Teaching Assistant: <br> 
                Intro to Computer Vision: 
                <a target="_blank" href="https://haosulab.github.io/intro-cv/WI22/index.html">CSE152A WI22</a>, 
                <a target="_blank" href="https://cseweb.ucsd.edu/classes/sp19/cse152-a/">CSE152A SP19</a>, 
                <a target="_blank" href="https://cseweb.ucsd.edu/~mkchandraker/classes/CSE152A/Winter2019/">CSE152A WI19</a> <br>
                Advanced Computer Vision: 
                <a target="_blank" href="https://cseweb.ucsd.edu/~mkchandraker/classes/CSE252D/Spring2021/">CSE252D SP21</a> <br>
                Domain Adaptation in Computer Vision: 
                <a target="_blank" href="https://cseweb.ucsd.edu/~mkchandraker/classes/CSE291/Winter2020/">CSE291A00 WI20</a> <br>
                <br>
                Honors and Awards: <br>
                <a target="_blank" href="https://research.facebook.com/blog/2022/2/announcing-the-recipients-of-the-2022-meta-phd-research-fellowship/">2022 Meta PhD Research Fellowship Finalist</a><br>
                <a target="_blank" href="https://www.qualcomm.com/research/university-relations/innovation/fellowship/2022-north-america">2022 Qualcomm Innovative Fellowship Finalist</a><br>
                <a target="_blank" href="https://research.google/outreach/phd-fellowship/recipients/">2022 Google PhD Fellowship Recipient</a> [<a target="_blank" href="https://cse.ucsd.edu/about/news/yu-ying-yeh-phd-24-cses-newest-recipient-prestigious-google-fellowship">CSE News</a>]
                
                <br>
              </td>
            </tr>
            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  This page is adapted from this <a target="_blank" href="https://github.com/jonbarron/website">template</a> which is created by <a target="_blank" href="http://jonbarron.info">Jon Barron</a>.
                </p>
                <p style="text-align:center;font-size:small;">
                
                  <a target="_blank" href="https://clustrmaps.com/site/1b0l1" title="Visit tracker"><img src="https://www.clustrmaps.com/map_v2.png?d=sI3uXC17N02IXnLan89Vlgs6IFTeeVZFpS9l1mALqN0&cl=786b6f&co=ffffff&ct=808080&cmo=cdaa9d&cmn=d6818d"></a>
                  
                  <!-- 
                  <a href="https://clustrmaps.com/site/1b0l1" title="Visit tracker"> </a>
                  -->
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
